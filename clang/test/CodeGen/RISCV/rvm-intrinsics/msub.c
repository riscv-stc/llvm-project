// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple riscv64 -target-feature +experimental-matrix -target-feature +experimental-v -target-feature +f -target-feature +d -disable-O0-optnone -emit-llvm %s -o - | opt -S -mem2reg | FileCheck --check-prefix=CHECK-IR-RV64 %s


#include <riscv_matrix.h>

// CHECK-IR-RV64-LABEL: @test_msub_mm_int8m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i8* [[IN1:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mlc.m.nxv128i8.i64(<vscale x 128 x i8>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4:[0-9]+]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = bitcast i8* [[IN2:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mlc.m.nxv128i8.i64(<vscale x 128 x i8>* [[TMP2]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP4:%.*]] = call <vscale x 128 x i8> @llvm.riscv.msub.mm.nxv128i8(<vscale x 128 x i8> [[TMP1]], <vscale x 128 x i8> [[TMP3]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP5:%.*]] = call <vscale x 128 x i8> @llvm.riscv.msub.b.mm.nxv128i8(<vscale x 128 x i8> [[TMP4]], <vscale x 128 x i8> [[TMP3]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP6:%.*]] = bitcast i8* [[OUT:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv128i8.i64(<vscale x 128 x i8> [[TMP5]], <vscale x 128 x i8>* [[TMP6]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_msub_mm_int8m1(const int8_t *in1, const int8_t *in2, int8_t *out, size_t a) {
    mint8_t m1 = mlc_m(in1, a);
    mint8_t m2 = mlc_m(in2, a);
    mint8_t mo = msub_mm(m1, m2);
    mo = msub_b_mm(mo, m2);
    msc_m(mo, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_msub_mm_int16m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i16* [[IN1:%.*]] to <vscale x 64 x i16>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 64 x i16> @llvm.riscv.mlc.m.nxv64i16.i64(<vscale x 64 x i16>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = bitcast i16* [[IN2:%.*]] to <vscale x 64 x i16>*
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = call <vscale x 64 x i16> @llvm.riscv.mlc.m.nxv64i16.i64(<vscale x 64 x i16>* [[TMP2]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP4:%.*]] = call <vscale x 64 x i16> @llvm.riscv.msub.mm.nxv64i16(<vscale x 64 x i16> [[TMP1]], <vscale x 64 x i16> [[TMP3]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP5:%.*]] = call <vscale x 64 x i16> @llvm.riscv.msub.h.mm.nxv64i16(<vscale x 64 x i16> [[TMP4]], <vscale x 64 x i16> [[TMP3]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP6:%.*]] = bitcast i16* [[OUT:%.*]] to <vscale x 64 x i16>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv64i16.i64(<vscale x 64 x i16> [[TMP5]], <vscale x 64 x i16>* [[TMP6]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_msub_mm_int16m1(const int16_t *in1, const int16_t *in2, int16_t *out, size_t a) {
    mint16_t m1 = mlc_m(in1, a);
    mint16_t m2 = mlc_m(in2, a);
    mint16_t mo = msub_mm(m1, m2);
    mo = msub_h_mm(mo, m2);
    msc_m(mo, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_msub_mm_int32m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i32* [[IN1:%.*]] to <vscale x 32 x i32>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 32 x i32> @llvm.riscv.mlc.m.nxv32i32.i64(<vscale x 32 x i32>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = bitcast i32* [[IN2:%.*]] to <vscale x 32 x i32>*
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = call <vscale x 32 x i32> @llvm.riscv.mlc.m.nxv32i32.i64(<vscale x 32 x i32>* [[TMP2]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP4:%.*]] = call <vscale x 32 x i32> @llvm.riscv.msub.mm.nxv32i32(<vscale x 32 x i32> [[TMP1]], <vscale x 32 x i32> [[TMP3]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP5:%.*]] = call <vscale x 32 x i32> @llvm.riscv.msub.w.mm.nxv32i32(<vscale x 32 x i32> [[TMP4]], <vscale x 32 x i32> [[TMP3]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP6:%.*]] = bitcast i32* [[OUT:%.*]] to <vscale x 32 x i32>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv32i32.i64(<vscale x 32 x i32> [[TMP5]], <vscale x 32 x i32>* [[TMP6]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_msub_mm_int32m1(const int32_t *in1, const int32_t *in2, int32_t *out, size_t a) {
    mint32_t m1 = mlc_m(in1, a);
    mint32_t m2 = mlc_m(in2, a);
    mint32_t mo = msub_mm(m1, m2);
    mo = msub_w_mm(mo, m2);
    msc_m(mo, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_msub_mm_int64m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i64* [[IN1:%.*]] to <vscale x 16 x i64>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 16 x i64> @llvm.riscv.mlc.m.nxv16i64.i64(<vscale x 16 x i64>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = bitcast i64* [[IN2:%.*]] to <vscale x 16 x i64>*
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = call <vscale x 16 x i64> @llvm.riscv.mlc.m.nxv16i64.i64(<vscale x 16 x i64>* [[TMP2]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP4:%.*]] = call <vscale x 16 x i64> @llvm.riscv.msub.mm.nxv16i64(<vscale x 16 x i64> [[TMP1]], <vscale x 16 x i64> [[TMP3]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP5:%.*]] = call <vscale x 16 x i64> @llvm.riscv.msub.dw.mm.nxv16i64(<vscale x 16 x i64> [[TMP4]], <vscale x 16 x i64> [[TMP3]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP6:%.*]] = bitcast i64* [[OUT:%.*]] to <vscale x 16 x i64>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv16i64.i64(<vscale x 16 x i64> [[TMP5]], <vscale x 16 x i64>* [[TMP6]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_msub_mm_int64m1(const int64_t *in1, const int64_t *in2, int64_t *out, size_t a) {
    mint64_t m1 = mlc_m(in1, a);
    mint64_t m2 = mlc_m(in2, a);
    mint64_t mo = msub_mm(m1, m2);
    mo = msub_dw_mm(mo, m2);
    msc_m(mo, out, a);
    return;
}
