// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple riscv64 -target-feature +experimental-matrix -target-feature +experimental-v -target-feature +f -target-feature +d -disable-O0-optnone -emit-llvm %s -o - | opt -S -mem2reg | FileCheck --check-prefix=CHECK-IR-RV64 %s


#include <riscv_matrix.h>

// CHECK-IR-RV64-LABEL: @test_mmvac_v_m_mint8_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i8* [[IN1:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mla.m.nxv128i8.i64(<vscale x 128 x i8>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4:[0-9]+]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 8 x i8> @llvm.riscv.mmvac.v.m.nxv8i8.nxv128i8.i64(<vscale x 128 x i8> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i8* [[OUT:%.*]] to <vscale x 8 x i8>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.v.nxv8i8.i64(<vscale x 8 x i8> [[TMP2]], <vscale x 8 x i8>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_v_m_mint8_m1(const int8_t *in1, int8_t *out, size_t a) {
    mint8_t m1 = mla_m(in1, a);
    vint8m1_t v1 = mmvac_v_m(m1, a);
    msa_v(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_v_m_muint8_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i8* [[IN1:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mla.m.nxv128i8.i64(<vscale x 128 x i8>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 8 x i8> @llvm.riscv.mmvac.v.m.nxv8i8.nxv128i8.i64(<vscale x 128 x i8> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i8* [[OUT:%.*]] to <vscale x 8 x i8>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.v.nxv8i8.i64(<vscale x 8 x i8> [[TMP2]], <vscale x 8 x i8>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_v_m_muint8_m1(const uint8_t *in1, uint8_t *out, size_t a) {
    muint8_t m1 = mla_m(in1, a);
    vuint8m1_t v1 = mmvac_v_m(m1, a);
    msa_v(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_v_m_mint16_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i16* [[IN1:%.*]] to <vscale x 64 x i16>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 64 x i16> @llvm.riscv.mla.m.nxv64i16.i64(<vscale x 64 x i16>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 4 x i16> @llvm.riscv.mmvac.v.m.nxv4i16.nxv64i16.i64(<vscale x 64 x i16> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i16* [[OUT:%.*]] to <vscale x 4 x i16>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.v.nxv4i16.i64(<vscale x 4 x i16> [[TMP2]], <vscale x 4 x i16>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_v_m_mint16_m1(const int16_t *in1, int16_t *out, size_t a) {
    mint16_t m1 = mla_m(in1, a);
    vint16m1_t v1 = mmvac_v_m(m1, a);
    msa_v(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_v_m_muint16_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i16* [[IN1:%.*]] to <vscale x 64 x i16>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 64 x i16> @llvm.riscv.mla.m.nxv64i16.i64(<vscale x 64 x i16>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 4 x i16> @llvm.riscv.mmvac.v.m.nxv4i16.nxv64i16.i64(<vscale x 64 x i16> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i16* [[OUT:%.*]] to <vscale x 4 x i16>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.v.nxv4i16.i64(<vscale x 4 x i16> [[TMP2]], <vscale x 4 x i16>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_v_m_muint16_m1(const uint16_t *in1, uint16_t *out, size_t a) {
    muint16_t m1 = mla_m(in1, a);
    vuint16m1_t v1 = mmvac_v_m(m1, a);
    msa_v(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_v_m_mint32_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i32* [[IN1:%.*]] to <vscale x 32 x i32>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 32 x i32> @llvm.riscv.mla.m.nxv32i32.i64(<vscale x 32 x i32>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 2 x i32> @llvm.riscv.mmvac.v.m.nxv2i32.nxv32i32.i64(<vscale x 32 x i32> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i32* [[OUT:%.*]] to <vscale x 2 x i32>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.v.nxv2i32.i64(<vscale x 2 x i32> [[TMP2]], <vscale x 2 x i32>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_v_m_mint32_m1(const int32_t *in1, int32_t *out, size_t a) {
    mint32_t m1 = mla_m(in1, a);
    vint32m1_t v1 = mmvac_v_m(m1, a);
    msa_v(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_v_m_muint32_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i32* [[IN1:%.*]] to <vscale x 32 x i32>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 32 x i32> @llvm.riscv.mla.m.nxv32i32.i64(<vscale x 32 x i32>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 2 x i32> @llvm.riscv.mmvac.v.m.nxv2i32.nxv32i32.i64(<vscale x 32 x i32> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i32* [[OUT:%.*]] to <vscale x 2 x i32>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.v.nxv2i32.i64(<vscale x 2 x i32> [[TMP2]], <vscale x 2 x i32>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_v_m_muint32_m1(const uint32_t *in1, uint32_t *out, size_t a) {
    muint32_t m1 = mla_m(in1, a);
    vuint32m1_t v1 = mmvac_v_m(m1, a);
    msa_v(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_v_m_mint64_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i64* [[IN1:%.*]] to <vscale x 16 x i64>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 16 x i64> @llvm.riscv.mla.m.nxv16i64.i64(<vscale x 16 x i64>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 1 x i64> @llvm.riscv.mmvac.v.m.nxv1i64.nxv16i64.i64(<vscale x 16 x i64> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i64* [[OUT:%.*]] to <vscale x 1 x i64>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.v.nxv1i64.i64(<vscale x 1 x i64> [[TMP2]], <vscale x 1 x i64>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_v_m_mint64_m1(const int64_t *in1, int64_t *out, size_t a) {
    mint64_t m1 = mla_m(in1, a);
    vint64m1_t v1 = mmvac_v_m(m1, a);
    msa_v(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_v_m_muint64_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i64* [[IN1:%.*]] to <vscale x 16 x i64>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 16 x i64> @llvm.riscv.mla.m.nxv16i64.i64(<vscale x 16 x i64>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 1 x i64> @llvm.riscv.mmvac.v.m.nxv1i64.nxv16i64.i64(<vscale x 16 x i64> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i64* [[OUT:%.*]] to <vscale x 1 x i64>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.v.nxv1i64.i64(<vscale x 1 x i64> [[TMP2]], <vscale x 1 x i64>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_v_m_muint64_m1(const uint64_t *in1, uint64_t *out, size_t a) {
    muint64_t m1 = mla_m(in1, a);
    vuint64m1_t v1 = mmvac_v_m(m1, a);
    msa_v(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_v_m_mfloat16_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast half* [[IN1:%.*]] to <vscale x 64 x half>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 64 x half> @llvm.riscv.mla.m.nxv64f16.i64(<vscale x 64 x half>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 4 x half> @llvm.riscv.mmvac.v.m.nxv4f16.nxv64f16.i64(<vscale x 64 x half> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast half* [[OUT:%.*]] to <vscale x 4 x half>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.v.nxv4f16.i64(<vscale x 4 x half> [[TMP2]], <vscale x 4 x half>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_v_m_mfloat16_m1(const _Float16 *in1, _Float16 *out, size_t a) {
    mfloat16_t m1 = mla_m(in1, a);
    vfloat16m1_t v1 = mmvac_v_m(m1, a);
    msa_v(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_v_m_mfloat32_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast float* [[IN1:%.*]] to <vscale x 32 x float>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 32 x float> @llvm.riscv.mla.m.nxv32f32.i64(<vscale x 32 x float>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 2 x float> @llvm.riscv.mmvac.v.m.nxv2f32.nxv32f32.i64(<vscale x 32 x float> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast float* [[OUT:%.*]] to <vscale x 2 x float>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.v.nxv2f32.i64(<vscale x 2 x float> [[TMP2]], <vscale x 2 x float>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_v_m_mfloat32_m1(const float *in1, float *out, size_t a) {
    mfloat32_t m1 = mla_m(in1, a);
    vfloat32m1_t v1 = mmvac_v_m(m1, a);
    msa_v(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_v_m_mfloat64_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast double* [[IN1:%.*]] to <vscale x 16 x double>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 16 x double> @llvm.riscv.mla.m.nxv16f64.i64(<vscale x 16 x double>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 1 x double> @llvm.riscv.mmvac.v.m.nxv1f64.nxv16f64.i64(<vscale x 16 x double> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast double* [[OUT:%.*]] to <vscale x 1 x double>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.v.nxv1f64.i64(<vscale x 1 x double> [[TMP2]], <vscale x 1 x double>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_v_m_mfloat64_m1(const double *in1, double *out, size_t a) {
    mfloat64_t m1 = mla_m(in1, a);
    vfloat64m1_t v1 = mmvac_v_m(m1, a);
    msa_v(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_m_v_mint8_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i8* [[IN1:%.*]] to <vscale x 8 x i8>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 8 x i8> @llvm.riscv.mla.v.nxv8i8.i64(<vscale x 8 x i8>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mmvac.m.v.nxv128i8.nxv8i8.i64(<vscale x 8 x i8> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i8* [[OUT:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.m.nxv128i8.i64(<vscale x 128 x i8> [[TMP2]], <vscale x 128 x i8>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_m_v_mint8_m1(const int8_t *in1, int8_t *out, size_t a) {
    vint8m1_t m1 = mla_v(in1, a);
    mint8_t v1 = mmvac_m_v(m1, a);
    msa_m(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_m_v_muint8_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i8* [[IN1:%.*]] to <vscale x 8 x i8>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 8 x i8> @llvm.riscv.mla.v.nxv8i8.i64(<vscale x 8 x i8>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mmvac.m.v.nxv128i8.nxv8i8.i64(<vscale x 8 x i8> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i8* [[OUT:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.m.nxv128i8.i64(<vscale x 128 x i8> [[TMP2]], <vscale x 128 x i8>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_m_v_muint8_m1(const uint8_t *in1, uint8_t *out, size_t a) {
    vuint8m1_t m1 = mla_v(in1, a);
    muint8_t v1 = mmvac_m_v(m1, a);
    msa_m(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_m_v_mint16_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i16* [[IN1:%.*]] to <vscale x 4 x i16>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 4 x i16> @llvm.riscv.mla.v.nxv4i16.i64(<vscale x 4 x i16>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 64 x i16> @llvm.riscv.mmvac.m.v.nxv64i16.nxv4i16.i64(<vscale x 4 x i16> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i16* [[OUT:%.*]] to <vscale x 64 x i16>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.m.nxv64i16.i64(<vscale x 64 x i16> [[TMP2]], <vscale x 64 x i16>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_m_v_mint16_m1(const int16_t *in1, int16_t *out, size_t a) {
    vint16m1_t m1 = mla_v(in1, a);
    mint16_t v1 = mmvac_m_v(m1, a);
    msa_m(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_m_v_muint16_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i16* [[IN1:%.*]] to <vscale x 4 x i16>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 4 x i16> @llvm.riscv.mla.v.nxv4i16.i64(<vscale x 4 x i16>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 64 x i16> @llvm.riscv.mmvac.m.v.nxv64i16.nxv4i16.i64(<vscale x 4 x i16> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i16* [[OUT:%.*]] to <vscale x 64 x i16>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.m.nxv64i16.i64(<vscale x 64 x i16> [[TMP2]], <vscale x 64 x i16>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_m_v_muint16_m1(const uint16_t *in1, uint16_t *out, size_t a) {
    vuint16m1_t m1 = mla_v(in1, a);
    muint16_t v1 = mmvac_m_v(m1, a);
    msa_m(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_m_v_mint32_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i32* [[IN1:%.*]] to <vscale x 2 x i32>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 2 x i32> @llvm.riscv.mla.v.nxv2i32.i64(<vscale x 2 x i32>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 32 x i32> @llvm.riscv.mmvac.m.v.nxv32i32.nxv2i32.i64(<vscale x 2 x i32> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i32* [[OUT:%.*]] to <vscale x 32 x i32>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.m.nxv32i32.i64(<vscale x 32 x i32> [[TMP2]], <vscale x 32 x i32>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_m_v_mint32_m1(const int32_t *in1, int32_t *out, size_t a) {
    vint32m1_t m1 = mla_v(in1, a);
    mint32_t v1 = mmvac_m_v(m1, a);
    msa_m(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_m_v_muint32_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i32* [[IN1:%.*]] to <vscale x 2 x i32>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 2 x i32> @llvm.riscv.mla.v.nxv2i32.i64(<vscale x 2 x i32>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 32 x i32> @llvm.riscv.mmvac.m.v.nxv32i32.nxv2i32.i64(<vscale x 2 x i32> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i32* [[OUT:%.*]] to <vscale x 32 x i32>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.m.nxv32i32.i64(<vscale x 32 x i32> [[TMP2]], <vscale x 32 x i32>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_m_v_muint32_m1(const uint32_t *in1, uint32_t *out, size_t a) {
    vuint32m1_t m1 = mla_v(in1, a);
    muint32_t v1 = mmvac_m_v(m1, a);
    msa_m(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_m_v_mint64_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i64* [[IN1:%.*]] to <vscale x 1 x i64>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 1 x i64> @llvm.riscv.mla.v.nxv1i64.i64(<vscale x 1 x i64>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 16 x i64> @llvm.riscv.mmvac.m.v.nxv16i64.nxv1i64.i64(<vscale x 1 x i64> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i64* [[OUT:%.*]] to <vscale x 16 x i64>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.m.nxv16i64.i64(<vscale x 16 x i64> [[TMP2]], <vscale x 16 x i64>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_m_v_mint64_m1(const int64_t *in1, int64_t *out, size_t a) {
    vint64m1_t m1 = mla_v(in1, a);
    mint64_t v1 = mmvac_m_v(m1, a);
    msa_m(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_m_v_muint64_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i64* [[IN1:%.*]] to <vscale x 1 x i64>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 1 x i64> @llvm.riscv.mla.v.nxv1i64.i64(<vscale x 1 x i64>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 16 x i64> @llvm.riscv.mmvac.m.v.nxv16i64.nxv1i64.i64(<vscale x 1 x i64> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i64* [[OUT:%.*]] to <vscale x 16 x i64>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.m.nxv16i64.i64(<vscale x 16 x i64> [[TMP2]], <vscale x 16 x i64>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_m_v_muint64_m1(const uint64_t *in1, uint64_t *out, size_t a) {
    vuint64m1_t m1 = mla_v(in1, a);
    muint64_t v1 = mmvac_m_v(m1, a);
    msa_m(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_m_v_mfloat16_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast half* [[IN1:%.*]] to <vscale x 4 x half>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 4 x half> @llvm.riscv.mla.v.nxv4f16.i64(<vscale x 4 x half>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 64 x half> @llvm.riscv.mmvac.m.v.nxv64f16.nxv4f16.i64(<vscale x 4 x half> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast half* [[OUT:%.*]] to <vscale x 64 x half>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.m.nxv64f16.i64(<vscale x 64 x half> [[TMP2]], <vscale x 64 x half>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_m_v_mfloat16_m1(const _Float16 *in1, _Float16 *out, size_t a) {
    vfloat16m1_t m1 = mla_v(in1, a);
    mfloat16_t v1 = mmvac_m_v(m1, a);
    msa_m(v1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmvac_m_v_mfloat32_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast float* [[IN1:%.*]] to <vscale x 2 x float>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 2 x float> @llvm.riscv.mla.v.nxv2f32.i64(<vscale x 2 x float>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 32 x float> @llvm.riscv.mmvac.m.v.nxv32f32.nxv2f32.i64(<vscale x 2 x float> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast float* [[OUT:%.*]] to <vscale x 32 x float>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msa.m.nxv32f32.i64(<vscale x 32 x float> [[TMP2]], <vscale x 32 x float>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmvac_m_v_mfloat32_m1(const float *in1, float *out, size_t a) {
    vfloat32m1_t m1 = mla_v(in1, a);
    mfloat32_t v1 = mmvac_m_v(m1, a);
    msa_m(v1, out, a);
    return;
}
