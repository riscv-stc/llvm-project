// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple riscv64 -target-feature +experimental-matrix -target-feature +experimental-v -target-feature +f -target-feature +d -disable-O0-optnone -emit-llvm %s -o - | opt -S -mem2reg | FileCheck --check-prefix=CHECK-IR-RV64 %s


#include <riscv_matrix.h>

// CHECK-IR-RV64-LABEL: @test_mmv_s_x_mint8_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i8* [[IN1:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mlae.m.nxv128i8.i64(<vscale x 128 x i8>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4:[0-9]+]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mmv.s.x.nxv128i8.i8.i64(<vscale x 128 x i8> [[TMP1]], i8 5, i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i8* [[OUT:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msae.m.nxv128i8.i64(<vscale x 128 x i8> [[TMP2]], <vscale x 128 x i8>* [[TMP3]], i64 [[A]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmv_s_x_mint8_m1(const int8_t *in1, int8_t *out, size_t a) {
    mint8m1_t m1 = mlae8_m1(in1, a);
    m1 = mmv_s_x(m1, 5, a);
    msae8_m(m1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmv_s_x_muint8_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i8* [[IN1:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mlae.m.nxv128i8.i64(<vscale x 128 x i8>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mmv.s.x.nxv128i8.i8.i64(<vscale x 128 x i8> [[TMP1]], i8 5, i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i8* [[OUT:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msae.m.nxv128i8.i64(<vscale x 128 x i8> [[TMP2]], <vscale x 128 x i8>* [[TMP3]], i64 [[A]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmv_s_x_muint8_m1(const uint8_t *in1, uint8_t *out, size_t a) {
    muint8m1_t m1 = mlae8_m1(in1, a);
    m1 = mmv_s_x(m1, 5, a);
    msae8_m(m1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmv_s_x_mint16_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i16* [[IN1:%.*]] to <vscale x 64 x i16>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 64 x i16> @llvm.riscv.mlae.m.nxv64i16.i64(<vscale x 64 x i16>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 64 x i16> @llvm.riscv.mmv.s.x.nxv64i16.i16.i64(<vscale x 64 x i16> [[TMP1]], i16 5, i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i16* [[OUT:%.*]] to <vscale x 64 x i16>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msae.m.nxv64i16.i64(<vscale x 64 x i16> [[TMP2]], <vscale x 64 x i16>* [[TMP3]], i64 [[A]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmv_s_x_mint16_m1(const int16_t *in1, int16_t *out, size_t a) {
    mint16m1_t m1 = mlae16_m1(in1, a);
    m1 = mmv_s_x(m1, 5, a);
    msae16_m(m1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmv_s_x_muint16_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i16* [[IN1:%.*]] to <vscale x 64 x i16>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 64 x i16> @llvm.riscv.mlae.m.nxv64i16.i64(<vscale x 64 x i16>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 64 x i16> @llvm.riscv.mmv.s.x.nxv64i16.i16.i64(<vscale x 64 x i16> [[TMP1]], i16 5, i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i16* [[OUT:%.*]] to <vscale x 64 x i16>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msae.m.nxv64i16.i64(<vscale x 64 x i16> [[TMP2]], <vscale x 64 x i16>* [[TMP3]], i64 [[A]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmv_s_x_muint16_m1(const uint16_t *in1, uint16_t *out, size_t a) {
    muint16m1_t m1 = mlae16_m1(in1, a);
    m1 = mmv_s_x(m1, 5, a);
    msae16_m(m1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmv_s_x_mint32_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i32* [[IN1:%.*]] to <vscale x 32 x i32>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 32 x i32> @llvm.riscv.mlae.m.nxv32i32.i64(<vscale x 32 x i32>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 32 x i32> @llvm.riscv.mmv.s.x.nxv32i32.i32.i64(<vscale x 32 x i32> [[TMP1]], i32 5, i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i32* [[OUT:%.*]] to <vscale x 32 x i32>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msae.m.nxv32i32.i64(<vscale x 32 x i32> [[TMP2]], <vscale x 32 x i32>* [[TMP3]], i64 [[A]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmv_s_x_mint32_m1(const int32_t *in1, int32_t *out, size_t a) {
    mint32m1_t m1 = mlae32_m1(in1, a);
    m1 = mmv_s_x(m1, 5, a);
    msae32_m(m1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmv_s_x_muint32_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i32* [[IN1:%.*]] to <vscale x 32 x i32>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 32 x i32> @llvm.riscv.mlae.m.nxv32i32.i64(<vscale x 32 x i32>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 32 x i32> @llvm.riscv.mmv.s.x.nxv32i32.i32.i64(<vscale x 32 x i32> [[TMP1]], i32 5, i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i32* [[OUT:%.*]] to <vscale x 32 x i32>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msae.m.nxv32i32.i64(<vscale x 32 x i32> [[TMP2]], <vscale x 32 x i32>* [[TMP3]], i64 [[A]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmv_s_x_muint32_m1(const uint32_t *in1, uint32_t *out, size_t a) {
    muint32m1_t m1 = mlae32_m1(in1, a);
    m1 = mmv_s_x(m1, 5, a);
    msae32_m(m1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmv_s_x_mint64_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i64* [[IN1:%.*]] to <vscale x 16 x i64>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 16 x i64> @llvm.riscv.mlae.m.nxv16i64.i64(<vscale x 16 x i64>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 16 x i64> @llvm.riscv.mmv.s.x.nxv16i64.i64.i64(<vscale x 16 x i64> [[TMP1]], i64 5, i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i64* [[OUT:%.*]] to <vscale x 16 x i64>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msae.m.nxv16i64.i64(<vscale x 16 x i64> [[TMP2]], <vscale x 16 x i64>* [[TMP3]], i64 [[A]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmv_s_x_mint64_m1(const int64_t *in1, int64_t *out, size_t a) {
    mint64m1_t m1 = mlae64_m1(in1, a);
    m1 = mmv_s_x(m1, 5, a);
    msae64_m(m1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmv_s_x_muint64_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i64* [[IN1:%.*]] to <vscale x 16 x i64>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 16 x i64> @llvm.riscv.mlae.m.nxv16i64.i64(<vscale x 16 x i64>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 16 x i64> @llvm.riscv.mmv.s.x.nxv16i64.i64.i64(<vscale x 16 x i64> [[TMP1]], i64 5, i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast i64* [[OUT:%.*]] to <vscale x 16 x i64>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msae.m.nxv16i64.i64(<vscale x 16 x i64> [[TMP2]], <vscale x 16 x i64>* [[TMP3]], i64 [[A]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mmv_s_x_muint64_m1(const uint64_t *in1, uint64_t *out, size_t a) {
    muint64m1_t m1 = mlae64_m1(in1, a);
    m1 = mmv_s_x(m1, 5, a);
    msae64_m(m1, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mmv_x_s_mint8_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i8* [[IN1:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mlae.m.nxv128i8.i64(<vscale x 128 x i8>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call i8 @llvm.riscv.mmv.x.s.i8.nxv128i8.i64(<vscale x 128 x i8> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret i8 [[TMP2]]
//
int8_t test_mmv_x_s_mint8_m1(const int8_t *in1, size_t a) {
    mint8m1_t m1 = mlae8_m1(in1, a);
    return mmv_x_s(m1, a);
}

// CHECK-IR-RV64-LABEL: @test_mmv_x_s_muint8_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i8* [[IN1:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mlae.m.nxv128i8.i64(<vscale x 128 x i8>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call i8 @llvm.riscv.mmv.x.s.i8.nxv128i8.i64(<vscale x 128 x i8> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret i8 [[TMP2]]
//
uint8_t test_mmv_x_s_muint8_m1(const uint8_t *in1, size_t a) {
    muint8m1_t m1 = mlae8_m1(in1, a);
    return mmv_x_s(m1, a);
}

// CHECK-IR-RV64-LABEL: @test_mmv_x_s_mint16_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i16* [[IN1:%.*]] to <vscale x 64 x i16>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 64 x i16> @llvm.riscv.mlae.m.nxv64i16.i64(<vscale x 64 x i16>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call i16 @llvm.riscv.mmv.x.s.i16.nxv64i16.i64(<vscale x 64 x i16> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret i16 [[TMP2]]
//
int16_t test_mmv_x_s_mint16_m1(const int16_t *in1, size_t a) {
    mint16m1_t m1 = mlae16_m1(in1, a);
    return mmv_x_s(m1, a);
}

// CHECK-IR-RV64-LABEL: @test_mmv_x_s_muint16_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i16* [[IN1:%.*]] to <vscale x 64 x i16>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 64 x i16> @llvm.riscv.mlae.m.nxv64i16.i64(<vscale x 64 x i16>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call i16 @llvm.riscv.mmv.x.s.i16.nxv64i16.i64(<vscale x 64 x i16> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret i16 [[TMP2]]
//
uint16_t test_mmv_x_s_muint16_m1(const uint16_t *in1, size_t a) {
    muint16m1_t m1 = mlae16_m1(in1, a);
    return mmv_x_s(m1, a);
}

// CHECK-IR-RV64-LABEL: @test_mmv_x_s_mint32_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i32* [[IN1:%.*]] to <vscale x 32 x i32>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 32 x i32> @llvm.riscv.mlae.m.nxv32i32.i64(<vscale x 32 x i32>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.mmv.x.s.i32.nxv32i32.i64(<vscale x 32 x i32> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret i32 [[TMP2]]
//
int32_t test_mmv_x_s_mint32_m1(const int32_t *in1, size_t a) {
    mint32m1_t m1 = mlae32_m1(in1, a);
    return mmv_x_s(m1, a);
}

// CHECK-IR-RV64-LABEL: @test_mmv_x_s_muint32_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i32* [[IN1:%.*]] to <vscale x 32 x i32>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 32 x i32> @llvm.riscv.mlae.m.nxv32i32.i64(<vscale x 32 x i32>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.mmv.x.s.i32.nxv32i32.i64(<vscale x 32 x i32> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_mmv_x_s_muint32_m1(const uint32_t *in1, size_t a) {
    muint32m1_t m1 = mlae32_m1(in1, a);
    return mmv_x_s(m1, a);
}

// CHECK-IR-RV64-LABEL: @test_mmv_x_s_mint64_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i64* [[IN1:%.*]] to <vscale x 16 x i64>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 16 x i64> @llvm.riscv.mlae.m.nxv16i64.i64(<vscale x 16 x i64>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call i64 @llvm.riscv.mmv.x.s.i64.nxv16i64.i64(<vscale x 16 x i64> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret i64 [[TMP2]]
//
int64_t test_mmv_x_s_mint64_m1(const int64_t *in1, size_t a) {
    mint64m1_t m1 = mlae64_m1(in1, a);
    return mmv_x_s(m1, a);
}

// CHECK-IR-RV64-LABEL: @test_mmv_x_s_muint64_m1(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i64* [[IN1:%.*]] to <vscale x 16 x i64>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 16 x i64> @llvm.riscv.mlae.m.nxv16i64.i64(<vscale x 16 x i64>* [[TMP0]], i64 [[A:%.*]], i64 0) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call i64 @llvm.riscv.mmv.x.s.i64.nxv16i64.i64(<vscale x 16 x i64> [[TMP1]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret i64 [[TMP2]]
//
uint64_t test_mmv_x_s_muint64_m1(const uint64_t *in1, size_t a) {
    muint64m1_t m1 = mlae64_m1(in1, a);
    return mmv_x_s(m1, a);
}
