// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple riscv64 -target-feature +experimental-matrix \
// RUN: -target-feature +experimental-v -target-feature +f -target-feature +d \
// RUN: -target-feature +experimental-zfh -disable-O0-optnone -emit-llvm %s -o - \
// RUN: | opt -S -mem2reg | FileCheck --check-prefix=CHECK-IR-RV64 %s


#include <riscv_matrix.h>

// CHECK-IR-RV64-LABEL: @test_mfwcvt_fw_x_m_f16(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i8* [[IN1:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mlc.m.nxv128i8.i64(<vscale x 128 x i8>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4:[0-9]+]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 64 x half> @llvm.riscv.mfwcvt.fw.x.m.nxv64f16.nxv128i8(<vscale x 128 x i8> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast half* [[OUT:%.*]] to <vscale x 64 x half>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv64f16.i64(<vscale x 64 x half> [[TMP2]], <vscale x 64 x half>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP4:%.*]] = call <vscale x 64 x half> @llvm.riscv.mfwcvt.hf.b.m.nxv64f16.nxv128i8(<vscale x 128 x i8> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP5:%.*]] = bitcast half* [[OUT]] to <vscale x 64 x half>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv64f16.i64(<vscale x 64 x half> [[TMP4]], <vscale x 64 x half>* [[TMP5]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mfwcvt_fw_x_m_f16(const _Float16 *out,  int8_t *in1, size_t a) {
    mint8_t m1 = mlc_m(in1, a);
    mfloat16_t mo = mfwcvt_fw_x_m(m1);
    msc_m(mo, out, a);
    mo = mfwcvt_hf_b_m(m1);
    msc_m(mo, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mfwcvt_fw_x_m_f32(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i16* [[IN1:%.*]] to <vscale x 64 x i16>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 64 x i16> @llvm.riscv.mlc.m.nxv64i16.i64(<vscale x 64 x i16>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 32 x float> @llvm.riscv.mfwcvt.fw.x.m.nxv32f32.nxv64i16(<vscale x 64 x i16> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast float* [[OUT:%.*]] to <vscale x 32 x float>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv32f32.i64(<vscale x 32 x float> [[TMP2]], <vscale x 32 x float>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP4:%.*]] = call <vscale x 32 x float> @llvm.riscv.mfwcvt.f.h.m.nxv32f32.nxv64i16(<vscale x 64 x i16> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP5:%.*]] = bitcast float* [[OUT]] to <vscale x 32 x float>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv32f32.i64(<vscale x 32 x float> [[TMP4]], <vscale x 32 x float>* [[TMP5]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mfwcvt_fw_x_m_f32(const float *out,  int16_t *in1, size_t a) {
    mint16_t m1 = mlc_m(in1, a);
    mfloat32_t mo = mfwcvt_fw_x_m(m1);
    msc_m(mo, out, a);
    mo = mfwcvt_f_h_m(m1);
    msc_m(mo, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mfwcvt_fw_x_m_f64(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i32* [[IN1:%.*]] to <vscale x 32 x i32>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 32 x i32> @llvm.riscv.mlc.m.nxv32i32.i64(<vscale x 32 x i32>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 16 x double> @llvm.riscv.mfwcvt.fw.x.m.nxv16f64.nxv32i32(<vscale x 32 x i32> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast double* [[OUT:%.*]] to <vscale x 16 x double>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv16f64.i64(<vscale x 16 x double> [[TMP2]], <vscale x 16 x double>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP4:%.*]] = call <vscale x 16 x double> @llvm.riscv.mfwcvt.d.w.m.nxv16f64.nxv32i32(<vscale x 32 x i32> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP5:%.*]] = bitcast double* [[OUT]] to <vscale x 16 x double>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv16f64.i64(<vscale x 16 x double> [[TMP4]], <vscale x 16 x double>* [[TMP5]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mfwcvt_fw_x_m_f64(const double *out,  int32_t *in1, size_t a) {
    mint32_t m1 = mlc_m(in1, a);
    mfloat64_t mo = mfwcvt_fw_x_m(m1);
    msc_m(mo, out, a);
    mo = mfwcvt_d_w_m(m1);
    msc_m(mo, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mfwcvt_fq_x_m_f32(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i8* [[IN1:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mlc.m.nxv128i8.i64(<vscale x 128 x i8>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 32 x float> @llvm.riscv.mfwcvt.fq.x.m.nxv32f32.nxv128i8(<vscale x 128 x i8> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast float* [[OUT:%.*]] to <vscale x 32 x float>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv32f32.i64(<vscale x 32 x float> [[TMP2]], <vscale x 32 x float>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP4:%.*]] = call <vscale x 32 x float> @llvm.riscv.mfwcvt.f.b.m.nxv32f32.nxv128i8(<vscale x 128 x i8> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP5:%.*]] = bitcast float* [[OUT]] to <vscale x 32 x float>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv32f32.i64(<vscale x 32 x float> [[TMP4]], <vscale x 32 x float>* [[TMP5]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mfwcvt_fq_x_m_f32(const float *out,  int8_t *in1, size_t a) {
    mint8_t m1 = mlc_m(in1, a);
    mfloat32_t mo = mfwcvt_fq_x_m(m1);
    msc_m(mo, out, a);
    mo = mfwcvt_f_b_m(m1);
    msc_m(mo, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mfwcvtu_fw_x_m_f16(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i8* [[IN1:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mlc.m.nxv128i8.i64(<vscale x 128 x i8>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 64 x half> @llvm.riscv.mfwcvtu.fw.x.m.nxv64f16.nxv128i8(<vscale x 128 x i8> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast half* [[OUT:%.*]] to <vscale x 64 x half>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv64f16.i64(<vscale x 64 x half> [[TMP2]], <vscale x 64 x half>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP4:%.*]] = call <vscale x 64 x half> @llvm.riscv.mfwcvtu.hf.b.m.nxv64f16.nxv128i8(<vscale x 128 x i8> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP5:%.*]] = bitcast half* [[OUT]] to <vscale x 64 x half>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv64f16.i64(<vscale x 64 x half> [[TMP4]], <vscale x 64 x half>* [[TMP5]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mfwcvtu_fw_x_m_f16(const _Float16 *out,  uint8_t *in1, size_t a) {
    muint8_t m1 = mlc_m(in1, a);
    mfloat16_t mo = mfwcvtu_fw_x_m(m1);
    msc_m(mo, out, a);
    mo = mfwcvtu_hf_b_m(m1);
    msc_m(mo, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mfwcvtu_fw_x_m_f32(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i16* [[IN1:%.*]] to <vscale x 64 x i16>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 64 x i16> @llvm.riscv.mlc.m.nxv64i16.i64(<vscale x 64 x i16>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 32 x float> @llvm.riscv.mfwcvtu.fw.x.m.nxv32f32.nxv64i16(<vscale x 64 x i16> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast float* [[OUT:%.*]] to <vscale x 32 x float>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv32f32.i64(<vscale x 32 x float> [[TMP2]], <vscale x 32 x float>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP4:%.*]] = call <vscale x 32 x float> @llvm.riscv.mfwcvtu.f.h.m.nxv32f32.nxv64i16(<vscale x 64 x i16> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP5:%.*]] = bitcast float* [[OUT]] to <vscale x 32 x float>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv32f32.i64(<vscale x 32 x float> [[TMP4]], <vscale x 32 x float>* [[TMP5]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mfwcvtu_fw_x_m_f32(const float *out,  uint16_t *in1, size_t a) {
    muint16_t m1 = mlc_m(in1, a);
    mfloat32_t mo = mfwcvtu_fw_x_m(m1);
    msc_m(mo, out, a);
    mo = mfwcvtu_f_h_m(m1);
    msc_m(mo, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mfwcvtu_fw_x_m_f64(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i32* [[IN1:%.*]] to <vscale x 32 x i32>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 32 x i32> @llvm.riscv.mlc.m.nxv32i32.i64(<vscale x 32 x i32>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 16 x double> @llvm.riscv.mfwcvtu.fw.x.m.nxv16f64.nxv32i32(<vscale x 32 x i32> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast double* [[OUT:%.*]] to <vscale x 16 x double>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv16f64.i64(<vscale x 16 x double> [[TMP2]], <vscale x 16 x double>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP4:%.*]] = call <vscale x 16 x double> @llvm.riscv.mfwcvtu.d.w.m.nxv16f64.nxv32i32(<vscale x 32 x i32> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP5:%.*]] = bitcast double* [[OUT]] to <vscale x 16 x double>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv16f64.i64(<vscale x 16 x double> [[TMP4]], <vscale x 16 x double>* [[TMP5]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mfwcvtu_fw_x_m_f64(const double *out,  uint32_t *in1, size_t a) {
    muint32_t m1 = mlc_m(in1, a);
    mfloat64_t mo = mfwcvtu_fw_x_m(m1);
    msc_m(mo, out, a);
    mo = mfwcvtu_d_w_m(m1);
    msc_m(mo, out, a);
    return;
}

// CHECK-IR-RV64-LABEL: @test_mfwcvtu_fq_x_m_f32(
// CHECK-IR-RV64-NEXT:  entry:
// CHECK-IR-RV64-NEXT:    [[TMP0:%.*]] = bitcast i8* [[IN1:%.*]] to <vscale x 128 x i8>*
// CHECK-IR-RV64-NEXT:    [[TMP1:%.*]] = call <vscale x 128 x i8> @llvm.riscv.mlc.m.nxv128i8.i64(<vscale x 128 x i8>* [[TMP0]], i64 [[A:%.*]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP2:%.*]] = call <vscale x 32 x float> @llvm.riscv.mfwcvtu.fq.x.m.nxv32f32.nxv128i8(<vscale x 128 x i8> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP3:%.*]] = bitcast float* [[OUT:%.*]] to <vscale x 32 x float>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv32f32.i64(<vscale x 32 x float> [[TMP2]], <vscale x 32 x float>* [[TMP3]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP4:%.*]] = call <vscale x 32 x float> @llvm.riscv.mfwcvtu.f.b.m.nxv32f32.nxv128i8(<vscale x 128 x i8> [[TMP1]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    [[TMP5:%.*]] = bitcast float* [[OUT]] to <vscale x 32 x float>*
// CHECK-IR-RV64-NEXT:    call void @llvm.riscv.msc.m.nxv32f32.i64(<vscale x 32 x float> [[TMP4]], <vscale x 32 x float>* [[TMP5]], i64 [[A]]) #[[ATTR4]]
// CHECK-IR-RV64-NEXT:    ret void
//
void test_mfwcvtu_fq_x_m_f32(const float *out,  uint8_t *in1, size_t a) {
    muint8_t m1 = mlc_m(in1, a);
    mfloat32_t mo = mfwcvtu_fq_x_m(m1);
    msc_m(mo, out, a);
    mo = mfwcvtu_f_b_m(m1);
    msc_m(mo, out, a);
    return;
}
