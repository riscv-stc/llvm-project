; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+experimental-matrix -mattr=+experimental-v,+d,+experimental-zfh \
; RUN:   -verify-machineinstrs < %s | FileCheck %s

declare <vscale x 128 x i8> @llvm.riscv.mla.m.nxv8i8(
  <vscale x 128 x i8>*,
  i64, i64)

declare void @llvm.riscv.msa.m.nxv8i8(
  <vscale x 128 x i8>,
  <vscale x 128 x i8>*,
  i64, i64)

declare <vscale x 128 x i8> @llvm.riscv.madd.mm.nxv8i8(
  <vscale x 128 x i8>,
  <vscale x 128 x i8>,
  i64)


define void @intrinsic_mlae_madd_msae_nxv8i8(<vscale x 128 x i8>* %0, <vscale x 128 x i8>* %1,
; CHECK-LABEL: intrinsic_mlae_madd_msae_nxv8i8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    csrr t0, mlenb
; CHECK-NEXT:    slli t0, t0, 1
; CHECK-NEXT:    sub sp, sp, t0
; CHECK-NEXT:    csrr t0, mlenb
; CHECK-NEXT:    slli t0, t0, 1
; CHECK-NEXT:    add t0, sp, t0
; CHECK-NEXT:    ld t0, 32(t0)
; CHECK-NEXT:    csrr t1, mlenb
; CHECK-NEXT:    slli t1, t1, 1
; CHECK-NEXT:    add t1, sp, t1
; CHECK-NEXT:    ld t1, 24(t1)
; CHECK-NEXT:    csrr t2, mlenb
; CHECK-NEXT:    slli t2, t2, 1
; CHECK-NEXT:    add t2, sp, t2
; CHECK-NEXT:    ld t2, 16(t2)
; CHECK-NEXT:    mlae8.m tr0, (a0), t0, m1
; CHECK-NEXT:    mlae8.m tr1, (a1), t0, m1
; CHECK-NEXT:    mlae8.m tr2, (a2), t0, m1
; CHECK-NEXT:    csrr a1, mrlenb
; CHECK-NEXT:    csrr a2, mlenb
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    msre8.m tr2, (a2), a1, m1 # Unknown-size Folded Spill
; CHECK-NEXT:    mlae8.m tr3, (a3), t0, m1
; CHECK-NEXT:    mlae8.m tr2, (a4), t0, m1
; CHECK-NEXT:    csrr a1, mrlenb
; CHECK-NEXT:    msre8.m tr2, (sp), a1, m1 # Unknown-size Folded Spill
; CHECK-NEXT:    mlae8.m tr5, (a5), t0, m1
; CHECK-NEXT:    mlae8.m tr6, (a6), t0, m1
; CHECK-NEXT:    mlae8.m tr7, (a7), t0, m1
; CHECK-NEXT:    madd.mm tr2, tr0, tr1, m1
; CHECK-NEXT:    mlae8.m tr1, (t2), t0, m1
; CHECK-NEXT:    mlae8.m tr0, (t1), t0, m1
; CHECK-NEXT:    csrr a1, mrlenb
; CHECK-NEXT:    csrr a2, mlenb
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    mlre8.m tr4, (a2), a1, m1 # Unknown-size Folded Reload
; CHECK-NEXT:    madd.mm tr3, tr4, tr3, m1
; CHECK-NEXT:    csrr a1, mrlenb
; CHECK-NEXT:    mlre8.m tr4, (sp), a1, m1 # Unknown-size Folded Reload
; CHECK-NEXT:    madd.mm tr4, tr4, tr5, m1
; CHECK-NEXT:    madd.mm tr5, tr6, tr7, m1
; CHECK-NEXT:    madd.mm tr0, tr1, tr0, m1
; CHECK-NEXT:    madd.mm tr1, tr2, tr3, m1
; CHECK-NEXT:    madd.mm tr2, tr4, tr5, m1
; CHECK-NEXT:    madd.mm tr0, tr0, tr1, m1
; CHECK-NEXT:    madd.mm tr0, tr2, tr0, m1
; CHECK-NEXT:    msae8.m tr0, (a0), t0, m1
; CHECK-NEXT:    csrr a0, mlenb
; CHECK-NEXT:    slli a0, a0, 1
; CHECK-NEXT:    add sp, sp, a0
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
                                             <vscale x 128 x i8>* %2, <vscale x 128 x i8>* %3,
                                             <vscale x 128 x i8>* %4, <vscale x 128 x i8>* %5,
                                             <vscale x 128 x i8>* %6, <vscale x 128 x i8>* %7,
                                             <vscale x 128 x i8>* %8, <vscale x 128 x i8>* %9,
                                             i64 %10, i64 %11) nounwind {

entry:

  %12 = call <vscale x 128 x i8> @llvm.riscv.mla.m.nxv8i8(
    <vscale x 128 x i8>* %0,
    i64 %10, i64 0)

  %13 = call <vscale x 128 x i8> @llvm.riscv.mla.m.nxv8i8(
    <vscale x 128 x i8>* %1,
    i64 %10, i64 0)

  %14 = call <vscale x 128 x i8> @llvm.riscv.mla.m.nxv8i8(
    <vscale x 128 x i8>* %2,
    i64 %10, i64 0)

  %15 = call <vscale x 128 x i8> @llvm.riscv.mla.m.nxv8i8(
    <vscale x 128 x i8>* %3,
    i64 %10, i64 0)

  %16 = call <vscale x 128 x i8> @llvm.riscv.mla.m.nxv8i8(
    <vscale x 128 x i8>* %4,
    i64 %10, i64 0)

  %17 = call <vscale x 128 x i8> @llvm.riscv.mla.m.nxv8i8(
    <vscale x 128 x i8>* %5,
    i64 %10, i64 0)

  %18 = call <vscale x 128 x i8> @llvm.riscv.mla.m.nxv8i8(
    <vscale x 128 x i8>* %6,
    i64 %10, i64 0)

  %19 = call <vscale x 128 x i8> @llvm.riscv.mla.m.nxv8i8(
    <vscale x 128 x i8>* %7,
    i64 %10, i64 0)

  %20 = call <vscale x 128 x i8> @llvm.riscv.mla.m.nxv8i8(
    <vscale x 128 x i8>* %8,
    i64 %10, i64 0)

  %21 = call <vscale x 128 x i8> @llvm.riscv.mla.m.nxv8i8(
    <vscale x 128 x i8>* %9,
    i64 %10, i64 0)


  %22 = call <vscale x 128 x i8> @llvm.riscv.madd.mm.nxv8i8(
    <vscale x 128 x i8> %12,
    <vscale x 128 x i8> %13,
    i64 0)

  %23 = call <vscale x 128 x i8> @llvm.riscv.madd.mm.nxv8i8(
    <vscale x 128 x i8> %14,
    <vscale x 128 x i8> %15,
    i64 0)

  %24 = call <vscale x 128 x i8> @llvm.riscv.madd.mm.nxv8i8(
    <vscale x 128 x i8> %16,
    <vscale x 128 x i8> %17,
    i64 0)

  %25 = call <vscale x 128 x i8> @llvm.riscv.madd.mm.nxv8i8(
    <vscale x 128 x i8> %18,
    <vscale x 128 x i8> %19,
    i64 0)

  %26 = call <vscale x 128 x i8> @llvm.riscv.madd.mm.nxv8i8(
    <vscale x 128 x i8> %20,
    <vscale x 128 x i8> %21,
    i64 0)

  %27 = call <vscale x 128 x i8> @llvm.riscv.madd.mm.nxv8i8(
    <vscale x 128 x i8> %22,
    <vscale x 128 x i8> %23,
    i64 0)

  %28 = call <vscale x 128 x i8> @llvm.riscv.madd.mm.nxv8i8(
    <vscale x 128 x i8> %24,
    <vscale x 128 x i8> %25,
    i64 0)

  %29 = call <vscale x 128 x i8> @llvm.riscv.madd.mm.nxv8i8(
    <vscale x 128 x i8> %26,
    <vscale x 128 x i8> %27,
    i64 0)

  %30 = call <vscale x 128 x i8> @llvm.riscv.madd.mm.nxv8i8(
    <vscale x 128 x i8> %28,
    <vscale x 128 x i8> %29,
    i64 0)

  call void @llvm.riscv.msa.m.nxv8i8(
    <vscale x 128 x i8> %30,
    <vscale x 128 x i8>* %0,
    i64 %10, i64 0)

  ret void
}
