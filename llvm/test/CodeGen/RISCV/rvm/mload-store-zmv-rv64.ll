; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+experimental-matrix -mattr=+experimental-v,+d,+experimental-zfh \
; RUN:   -verify-machineinstrs < %s | FileCheck %s

declare <vscale x 8 x i8> @llvm.riscv.mla.v.nxv8i8(
  <vscale x 8 x i8>*,
  i64
)

declare void @llvm.riscv.msa.v.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  i64
)

declare <vscale x 4 x i16> @llvm.riscv.mla.v.nxv4i16(
  <vscale x 4 x i16>*,
  i64
)

declare void @llvm.riscv.msa.v.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  i64
)

declare <vscale x 4 x half> @llvm.riscv.mla.v.nxv4f16(
  <vscale x 4 x half>*,
  i64
)

declare void @llvm.riscv.msa.v.nxv4f16(
  <vscale x 4 x half>,
  <vscale x 4 x half>*,
  i64
)

declare <vscale x 2 x i32> @llvm.riscv.mla.v.nxv2i32(
  <vscale x 2 x i32>*,
  i64
)

declare void @llvm.riscv.msa.v.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  i64
)

declare <vscale x 2 x float> @llvm.riscv.mla.v.nxv2f32(
  <vscale x 2 x float>*,
  i64
)

declare void @llvm.riscv.msa.v.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  i64
)

declare <vscale x 1 x i64> @llvm.riscv.mla.v.nxv1i64(
  <vscale x 1 x i64>*,
  i64
)

declare void @llvm.riscv.msa.v.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  i64
)

declare <vscale x 1 x double> @llvm.riscv.mla.v.nxv1f64(
  <vscale x 1 x double>*,
  i64
)

declare void @llvm.riscv.msa.v.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64
)

define void @intrinsic_mlav_msav(<vscale x 8 x i8>* %0, <vscale x 4 x i16>* %1, <vscale x 4 x half>* %2, <vscale x 2 x i32>* %3, <vscale x 2 x float>* %4, <vscale x 1 x i64>* %5, <vscale x 1 x double>* %6, i64 %7) nounwind {
; CHECK-LABEL: intrinsic_mlav_msav:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    mlae8.v v25, (a0), a7
; CHECK-NEXT:    msae8.v v25, (a0), a7
; CHECK-NEXT:    mlae16.v v25, (a1), a7
; CHECK-NEXT:    msae16.v v25, (a1), a7
; CHECK-NEXT:    mlae16.v v25, (a2), a7
; CHECK-NEXT:    msae16.v v25, (a2), a7
; CHECK-NEXT:    mlae32.v v25, (a3), a7
; CHECK-NEXT:    msae32.v v25, (a3), a7
; CHECK-NEXT:    mlae32.v v25, (a4), a7
; CHECK-NEXT:    msae32.v v25, (a4), a7
; CHECK-NEXT:    mlae64.v v25, (a5), a7
; CHECK-NEXT:    msae64.v v25, (a5), a7
; CHECK-NEXT:    mlae64.v v25, (a6), a7
; CHECK-NEXT:    msae64.v v25, (a6), a7
; CHECK-NEXT:    ret
entry:
  %8 = call <vscale x 8 x i8> @llvm.riscv.mla.v.nxv8i8(
    <vscale x 8 x i8>* %0,
    i64 %7
  )

  call void @llvm.riscv.msa.v.nxv8i8(
    <vscale x 8 x i8> %8,
    <vscale x 8 x i8>* %0,
    i64 %7
  )

  %9 = call <vscale x 4 x i16> @llvm.riscv.mla.v.nxv4i16(
    <vscale x 4 x i16>* %1,
    i64 %7
  )

  call void @llvm.riscv.msa.v.nxv4i16(
    <vscale x 4 x i16> %9,
    <vscale x 4 x i16>* %1,
    i64 %7
  )

  %10 = call <vscale x 4 x half> @llvm.riscv.mla.v.nxv4f16(
    <vscale x 4 x half>* %2,
    i64 %7
  )

  call void @llvm.riscv.msa.v.nxv4f16(
    <vscale x 4 x half> %10,
    <vscale x 4 x half>* %2,
    i64 %7
  )

  %11 = call <vscale x 2 x i32> @llvm.riscv.mla.v.nxv2i32(
    <vscale x 2 x i32>* %3,
    i64 %7
  )

  call void @llvm.riscv.msa.v.nxv2i32(
    <vscale x 2 x i32> %11,
    <vscale x 2 x i32>* %3,
    i64 %7
  )

  %12 = call <vscale x 2 x float> @llvm.riscv.mla.v.nxv2f32(
    <vscale x 2 x float>* %4,
    i64 %7
  )

  call void @llvm.riscv.msa.v.nxv2f32(
    <vscale x 2 x float> %12,
    <vscale x 2 x float>* %4,
    i64 %7
  )

  %13 = call <vscale x 1 x i64> @llvm.riscv.mla.v.nxv1i64(
    <vscale x 1 x i64>* %5,
    i64 %7
  )

  call void @llvm.riscv.msa.v.nxv1i64(
    <vscale x 1 x i64> %13,
    <vscale x 1 x i64>* %5,
    i64 %7
  )

  %14 = call <vscale x 1 x double> @llvm.riscv.mla.v.nxv1f64(
    <vscale x 1 x double>* %6,
    i64 %7
  )

  call void @llvm.riscv.msa.v.nxv1f64(
    <vscale x 1 x double> %14,
    <vscale x 1 x double>* %6,
    i64 %7
  )

  ret void
}

declare <vscale x 8 x i8> @llvm.riscv.mlb.v.nxv8i8(
  <vscale x 8 x i8>*,
  i64
)

declare void @llvm.riscv.msb.v.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  i64
)

declare <vscale x 4 x i16> @llvm.riscv.mlb.v.nxv4i16(
  <vscale x 4 x i16>*,
  i64
)

declare void @llvm.riscv.msb.v.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  i64
)

declare <vscale x 4 x half> @llvm.riscv.mlb.v.nxv4f16(
  <vscale x 4 x half>*,
  i64
)

declare void @llvm.riscv.msb.v.nxv4f16(
  <vscale x 4 x half>,
  <vscale x 4 x half>*,
  i64
)

declare <vscale x 2 x i32> @llvm.riscv.mlb.v.nxv2i32(
  <vscale x 2 x i32>*,
  i64
)

declare void @llvm.riscv.msb.v.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  i64
)

declare <vscale x 2 x float> @llvm.riscv.mlb.v.nxv2f32(
  <vscale x 2 x float>*,
  i64
)

declare void @llvm.riscv.msb.v.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  i64
)

declare <vscale x 1 x i64> @llvm.riscv.mlb.v.nxv1i64(
  <vscale x 1 x i64>*,
  i64
)

declare void @llvm.riscv.msb.v.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  i64
)

declare <vscale x 1 x double> @llvm.riscv.mlb.v.nxv1f64(
  <vscale x 1 x double>*,
  i64
)

declare void @llvm.riscv.msb.v.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64
)

define void @intrinsic_mlbv_msbv(<vscale x 8 x i8>* %0, <vscale x 4 x i16>* %1, <vscale x 4 x half>* %2, <vscale x 2 x i32>* %3, <vscale x 2 x float>* %4, <vscale x 1 x i64>* %5, <vscale x 1 x double>* %6, i64 %7) nounwind {
; CHECK-LABEL: intrinsic_mlbv_msbv:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    mlbe8.v v25, (a0), a7
; CHECK-NEXT:    msbe8.v v25, (a0), a7
; CHECK-NEXT:    mlbe16.v v25, (a1), a7
; CHECK-NEXT:    msbe16.v v25, (a1), a7
; CHECK-NEXT:    mlbe16.v v25, (a2), a7
; CHECK-NEXT:    msbe16.v v25, (a2), a7
; CHECK-NEXT:    mlbe32.v v25, (a3), a7
; CHECK-NEXT:    msbe32.v v25, (a3), a7
; CHECK-NEXT:    mlbe32.v v25, (a4), a7
; CHECK-NEXT:    msbe32.v v25, (a4), a7
; CHECK-NEXT:    mlbe64.v v25, (a5), a7
; CHECK-NEXT:    msbe64.v v25, (a5), a7
; CHECK-NEXT:    mlbe64.v v25, (a6), a7
; CHECK-NEXT:    msbe64.v v25, (a6), a7
; CHECK-NEXT:    ret
entry:
  %8 = call <vscale x 8 x i8> @llvm.riscv.mlb.v.nxv8i8(
    <vscale x 8 x i8>* %0,
    i64 %7
  )

  call void @llvm.riscv.msb.v.nxv8i8(
    <vscale x 8 x i8> %8,
    <vscale x 8 x i8>* %0,
    i64 %7
  )

  %9 = call <vscale x 4 x i16> @llvm.riscv.mlb.v.nxv4i16(
    <vscale x 4 x i16>* %1,
    i64 %7
  )

  call void @llvm.riscv.msb.v.nxv4i16(
    <vscale x 4 x i16> %9,
    <vscale x 4 x i16>* %1,
    i64 %7
  )

  %10 = call <vscale x 4 x half> @llvm.riscv.mlb.v.nxv4f16(
    <vscale x 4 x half>* %2,
    i64 %7
  )

  call void @llvm.riscv.msb.v.nxv4f16(
    <vscale x 4 x half> %10,
    <vscale x 4 x half>* %2,
    i64 %7
  )

  %11 = call <vscale x 2 x i32> @llvm.riscv.mlb.v.nxv2i32(
    <vscale x 2 x i32>* %3,
    i64 %7
  )

  call void @llvm.riscv.msb.v.nxv2i32(
    <vscale x 2 x i32> %11,
    <vscale x 2 x i32>* %3,
    i64 %7
  )

  %12 = call <vscale x 2 x float> @llvm.riscv.mlb.v.nxv2f32(
    <vscale x 2 x float>* %4,
    i64 %7
  )

  call void @llvm.riscv.msb.v.nxv2f32(
    <vscale x 2 x float> %12,
    <vscale x 2 x float>* %4,
    i64 %7
  )

  %13 = call <vscale x 1 x i64> @llvm.riscv.mlb.v.nxv1i64(
    <vscale x 1 x i64>* %5,
    i64 %7
  )

  call void @llvm.riscv.msb.v.nxv1i64(
    <vscale x 1 x i64> %13,
    <vscale x 1 x i64>* %5,
    i64 %7
  )

  %14 = call <vscale x 1 x double> @llvm.riscv.mlb.v.nxv1f64(
    <vscale x 1 x double>* %6,
    i64 %7
  )

  call void @llvm.riscv.msb.v.nxv1f64(
    <vscale x 1 x double> %14,
    <vscale x 1 x double>* %6,
    i64 %7
  )

  ret void
}

declare <vscale x 8 x i8> @llvm.riscv.mlc.v.nxv8i8(
  <vscale x 8 x i8>*,
  i64
)

declare void @llvm.riscv.msc.v.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  i64
)

declare <vscale x 4 x i16> @llvm.riscv.mlc.v.nxv4i16(
  <vscale x 4 x i16>*,
  i64
)

declare void @llvm.riscv.msc.v.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  i64
)

declare <vscale x 4 x half> @llvm.riscv.mlc.v.nxv4f16(
  <vscale x 4 x half>*,
  i64
)

declare void @llvm.riscv.msc.v.nxv4f16(
  <vscale x 4 x half>,
  <vscale x 4 x half>*,
  i64
)

declare <vscale x 2 x i32> @llvm.riscv.mlc.v.nxv2i32(
  <vscale x 2 x i32>*,
  i64
)

declare void @llvm.riscv.msc.v.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  i64
)

declare <vscale x 2 x float> @llvm.riscv.mlc.v.nxv2f32(
  <vscale x 2 x float>*,
  i64
)

declare void @llvm.riscv.msc.v.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  i64
)

declare <vscale x 1 x i64> @llvm.riscv.mlc.v.nxv1i64(
  <vscale x 1 x i64>*,
  i64
)

declare void @llvm.riscv.msc.v.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  i64
)

declare <vscale x 1 x double> @llvm.riscv.mlc.v.nxv1f64(
  <vscale x 1 x double>*,
  i64
)

declare void @llvm.riscv.msc.v.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64
)

define void @intrinsic_mlcv_mscv(<vscale x 8 x i8>* %0, <vscale x 4 x i16>* %1, <vscale x 4 x half>* %2, <vscale x 2 x i32>* %3, <vscale x 2 x float>* %4, <vscale x 1 x i64>* %5, <vscale x 1 x double>* %6, i64 %7) nounwind {
; CHECK-LABEL: intrinsic_mlcv_mscv:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    mlce8.v v25, (a0), a7
; CHECK-NEXT:    msce8.v v25, (a0), a7
; CHECK-NEXT:    mlce16.v v25, (a1), a7
; CHECK-NEXT:    msce16.v v25, (a1), a7
; CHECK-NEXT:    mlce16.v v25, (a2), a7
; CHECK-NEXT:    msce16.v v25, (a2), a7
; CHECK-NEXT:    mlce32.v v25, (a3), a7
; CHECK-NEXT:    msce32.v v25, (a3), a7
; CHECK-NEXT:    mlce32.v v25, (a4), a7
; CHECK-NEXT:    msce32.v v25, (a4), a7
; CHECK-NEXT:    mlce64.v v25, (a5), a7
; CHECK-NEXT:    msce64.v v25, (a5), a7
; CHECK-NEXT:    mlce64.v v25, (a6), a7
; CHECK-NEXT:    msce64.v v25, (a6), a7
; CHECK-NEXT:    ret
entry:
  %8 = call <vscale x 8 x i8> @llvm.riscv.mlc.v.nxv8i8(
    <vscale x 8 x i8>* %0,
    i64 %7
  )

  call void @llvm.riscv.msc.v.nxv8i8(
    <vscale x 8 x i8> %8,
    <vscale x 8 x i8>* %0,
    i64 %7
  )

  %9 = call <vscale x 4 x i16> @llvm.riscv.mlc.v.nxv4i16(
    <vscale x 4 x i16>* %1,
    i64 %7
  )

  call void @llvm.riscv.msc.v.nxv4i16(
    <vscale x 4 x i16> %9,
    <vscale x 4 x i16>* %1,
    i64 %7
  )

  %10 = call <vscale x 4 x half> @llvm.riscv.mlc.v.nxv4f16(
    <vscale x 4 x half>* %2,
    i64 %7
  )

  call void @llvm.riscv.msc.v.nxv4f16(
    <vscale x 4 x half> %10,
    <vscale x 4 x half>* %2,
    i64 %7
  )

  %11 = call <vscale x 2 x i32> @llvm.riscv.mlc.v.nxv2i32(
    <vscale x 2 x i32>* %3,
    i64 %7
  )

  call void @llvm.riscv.msc.v.nxv2i32(
    <vscale x 2 x i32> %11,
    <vscale x 2 x i32>* %3,
    i64 %7
  )

  %12 = call <vscale x 2 x float> @llvm.riscv.mlc.v.nxv2f32(
    <vscale x 2 x float>* %4,
    i64 %7
  )

  call void @llvm.riscv.msc.v.nxv2f32(
    <vscale x 2 x float> %12,
    <vscale x 2 x float>* %4,
    i64 %7
  )

  %13 = call <vscale x 1 x i64> @llvm.riscv.mlc.v.nxv1i64(
    <vscale x 1 x i64>* %5,
    i64 %7
  )

  call void @llvm.riscv.msc.v.nxv1i64(
    <vscale x 1 x i64> %13,
    <vscale x 1 x i64>* %5,
    i64 %7
  )

  %14 = call <vscale x 1 x double> @llvm.riscv.mlc.v.nxv1f64(
    <vscale x 1 x double>* %6,
    i64 %7
  )

  call void @llvm.riscv.msc.v.nxv1f64(
    <vscale x 1 x double> %14,
    <vscale x 1 x double>* %6,
    i64 %7
  )

  ret void
}
